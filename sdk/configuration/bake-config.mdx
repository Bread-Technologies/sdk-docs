---
title: "Bake Configuration"
description: "Complete reference for bake (training) configuration"
---

## Overview

Bake configuration controls model training behavior, including datasets, training parameters, model adapters, and integrations.

## Core Configuration

### Datasets

<ParamField path="datasets" type="array" required>
  List of targets to use as training data
  
  ```python
  "datasets": [
      {"target": "coding_target", "weight": 0.7},
      {"target": "math_target", "weight": 0.3}
  ]
  ```
  
  Each dataset has:
  - `target` (string, required): Target name
  - `weight` (float): Sampling weight (higher = more frequently sampled)
</ParamField>

### Training Parameters

<ParamField path="epochs" type="integer">
  Number of training epochs
  
  ```python
  "epochs": 3
  ```
</ParamField>

<ParamField path="micro_batch_size" type="integer">
  Micro batch size
  
  ```python
  "micro_batch_size": 1
  ```
</ParamField>

<ParamField path="gradient_accumulation_steps" type="integer">
  Gradient accumulation steps for effective batch size
  
  ```python
  "gradient_accumulation_steps": 4
  ```
</ParamField>

<ParamField path="total_trajectories" type="integer">
  Total number of trajectories to use for training
  
  ```python
  "total_trajectories": 1000
  ```
</ParamField>

<ParamField path="seed" type="integer">
  Random seed for reproducibility
  
  ```python
  "seed": 42
  ```
</ParamField>

## Model Configuration

<ParamField path="model" type="object">
  Model and adapter configuration
  
  ```python
  "model": {
      "type": "bake",
      "parent_model_name": "Qwen/Qwen3-32B",
      "baked_adapter_config": {
          "r": 8,
          "lora_alpha": 16,
          "lora_dropout": 0.05,
          "bias": "none",
          "target_modules": "all-linear"
      }
  }
  ```
  
  **Fields**:
  - `type`: Model type (e.g., `"bake"`)
  - `parent_model_name`: Parent model name (base model like `"Qwen/Qwen3-32B"` or baked model like `"user/repo/bake_name/checkpoint"`). Defaults to the repository's base model if not specified.
  - `baked_adapter_config`: LoRA configuration (see below)
  - `dtype`: Data type for model weights (e.g., `"bf16"`, `"fp16"`, `"fp32"`)
  - `attn_implementation`: Attention implementation (e.g., `"sdpa"`, `"flash_attention_2"`)
  - `disable_activation_checkpoint`: Disable the use of activation checkpointing (default: `false`)
  - `peft_config`: Configuration dictionary for Parameter Efficient Fine Tuning
</ParamField>

### LoRA Configuration

<ParamField path="model.baked_adapter_config" type="object">
  LoRA (Low-Rank Adaptation) configuration
  
  ```python
  "baked_adapter_config": {
      "r": 8,                    # LoRA rank
      "lora_alpha": 16,          # Alpha parameter
      "lora_dropout": 0.05,      # Dropout rate
      "bias": "none",            # Bias handling
      "target_modules": "all-linear"  # Target modules
  }
  ```
</ParamField>

## Optimizer & Scheduler

<ParamField path="optimizer" type="object">
  Optimizer configuration
  
  ```python
  "optimizer": {
      "learning_rate": 0.0001
  }
  ```
</ParamField>

<ParamField path="scheduler" type="object">
  Learning rate scheduler
  
  ```python
  "scheduler": {
      "type": "huggingface"
  }
  ```
</ParamField>

## Advanced Configuration

### DeepSpeed

<ParamField path="deepspeed" type="object">
  DeepSpeed ZeRO configuration
  
  ```python
  "deepspeed": {
      "zero_optimization": {
          "stage": 2
      }
  }
  ```
  
  **ZeRO Stages**:
  - Stage 0: Disabled
  - Stage 1: Optimizer state partitioning
  - Stage 2: + Gradient partitioning
  - Stage 3: + Parameter partitioning
</ParamField>

### Checkpoint Configuration

<ParamField path="checkpoint" type="array">
  List of checkpoint engine configurations
  
  ```python
  "checkpoint": [
      {
          "type": "huggingface",
          "output_dir": "./checkpoints",
          "enabled": True,
          "auto_resume": False,
          "save_every_n_steps": 1000,
          "save_every_n_epochs": 1,
          "save_end_of_training": True
      }
  ]
  ```
  
  **Fields**:
  - `type`: Checkpoint engine type (e.g., `"huggingface"`)
  - `output_dir`: Output directory for checkpoints
  - `enabled`: Enable this checkpoint engine (default: `True`)
  - `auto_resume`: Resume training from checkpoint if found (default: `False`)
  - `save_every_n_steps`: Save checkpoint every N training steps
  - `save_every_n_epochs`: Save checkpoint every N epochs
  - `save_end_of_training`: Save checkpoint at end of training (default: `False`)
</ParamField>

### Data Configuration

<ParamField path="data" type="object">
  Data loading and processing configuration
  
  ```python
  "data": {
      "type": "single_baker",
      "sources": [
          {
              "type": "bake_jsonl",
              "name_or_path": "data/train.jsonl",
              "split": "train",
              "max_samples": 10000
          }
      ],
      "eval_sources": [
          {
              "type": "bake_jsonl",
              "name_or_path": "data/eval.jsonl",
              "split": "eval"
          }
      ],
      "max_length": 2048,
      "train_eval_split": [0.9, 0.1],
      "dl_num_workers": 4,
      "num_proc": 8,
      "seed": 42
  }
  ```
  
  **Fields**:
  - `type`: Data type (e.g., `"single_baker"`)
  - `sources`: List of training data sources
  - `eval_sources`: List of evaluation data sources
  - `max_length`: Maximum sequence length
  - `train_eval_split`: Train/eval split ratio `[train, eval]` (must sum to 1.0)
  - `dl_num_workers`: Number of dataloader workers per GPU
  - `num_proc`: Number of processes for data loading
  - `seed`: Seed for data loading
  - `beta`: Beta parameter for training
  - `temperature`: Sampling temperature
</ParamField>

## Complete Example

<CodeGroup>

```python Python
bake = client.bakes.set(
    bake_name="production_bake",
    repo_name="my_repo",
    template="default",
    overrides={
        # Datasets
        "datasets": [
            {"target": "coding_target", "weight": 1.0}
        ],
        
        # Training params
        "epochs": 5,
        "micro_batch_size": 1,
        "gradient_accumulation_steps": 4,
        "total_trajectories": 10000,
        "seed": 42,
        
        # Model with LoRA
        "model": {
            "type": "bake",
            "parent_model_name": "Qwen/Qwen3-32B",
            "baked_adapter_config": {
                "r": 16,
                "lora_alpha": 32,
                "lora_dropout": 0.1,
                "bias": "none",
                "target_modules": "all-linear"
            }
        },
        
        # Optimizer
        "optimizer": {
            "learning_rate": 0.0001
        },
        
        # Scheduler
        "scheduler": {
            "type": "huggingface"
        },
        
        # DeepSpeed
        "deepspeed": {
            "zero_optimization": {
                "stage": 2
            }
        }
    }
)
```

```typescript TypeScript
const bake = await client.bakes.create('my_repo', {
  bake_name: 'production_bake',
  template: 'default',
  overrides: {
    // Datasets
    datasets: [
      { target: 'coding_target', weight: 1.0 },
    ],
    
    // Training params
    epochs: 5,
    micro_batch_size: 1,
    gradient_accumulation_steps: 4,
    total_trajectories: 10000,
    seed: 42,
    
    // Model with LoRA
    model: {
      type: 'bake',
      parent_model_name: 'Qwen/Qwen3-32B',
      baked_adapter_config: {
        r: 16,
        lora_alpha: 32,
        lora_dropout: 0.1,
        bias: 'none',
        target_modules: 'all-linear',
      },
    },
    
    // Optimizer
    optimizer: {
      learning_rate: 0.0001,
    },
    
    // Scheduler
    scheduler: {
      type: 'huggingface',
    },
    
    // DeepSpeed
    deepspeed: {
      zero_optimization: {
        stage: 2,
      },
    },
  },
});
```

</CodeGroup>

## Field Reference Table

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `datasets` | Array | Yes | Training data sources |
| `epochs` | Integer | No | Number of training epochs |
| `micro_batch_size` | Integer | No | Batch size per device |
| `gradient_accumulation_steps` | Integer | No | Gradient accumulation |
| `seed` | Integer | No | Random seed |
| `model` | Object | No | Model configuration |
| `data` | Object | No | Data loading configuration |
| `optimizer` | Object | No | Optimizer settings |
| `scheduler` | Object | No | LR scheduler |
| `deepspeed` | Object | No | DeepSpeed config |
| `checkpoint` | Array | No | Checkpoint engine configuration |

## Template Inheritance

<CodeGroup>

```python Python
# Base configuration
client.bakes.set(
    bake_name="base_bake",
    repo_name="my_repo",
    template="default",
    overrides={
        "epochs": 3,
        "micro_batch_size": 1
    }
)

# Inherit and override
client.bakes.set(
    bake_name="long_bake",
    repo_name="my_repo",
    template="base_bake",
    overrides={
        "epochs": 10  # Override epochs only
    }
)
```

```typescript TypeScript
// Base configuration
await client.bakes.create('my_repo', {
  bake_name: 'base_bake',
  template: 'default',
  overrides: {
    epochs: 3,
    micro_batch_size: 1,
  },
});

// Inherit and override
await client.bakes.create('my_repo', {
  bake_name: 'long_bake',
  template: 'base_bake',
  overrides: {
    epochs: 10,  // Override epochs only
  },
});
```

</CodeGroup>

## Next Steps

<CardGroup cols={2}>
  <Card
    title="Bakes API"
    icon="fire"
    href="/api-reference/bakes"
  >
    Bakes API reference
  </Card>
  <Card
    title="Production Patterns"
    icon="shield"
    href="/sdk/guides/production-patterns"
  >
    Training workflow examples
  </Card>
</CardGroup>

