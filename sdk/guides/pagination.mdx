---
title: "Pagination"
description: "Handle large datasets with pagination"
---

## Overview

Stim and rollout outputs support pagination for efficiently handling large datasets. Use `offset` and `limit` parameters to paginate through results.

## Pagination Parameters

<ParamField path="limit" type="integer">
  Maximum number of items to return per request (max 1000)
</ParamField>

<ParamField path="offset" type="integer">
  Starting position in the dataset (0-indexed)
</ParamField>

<ParamField path="has_more" type="boolean" output>
  Response field indicating if more data is available
</ParamField>

## Basic Pagination

<CodeGroup>

```python Python
# First page
output = client.targets.stim.get_output(
    target_name="my_target",
    repo_name="my_repo",
    limit=100,
    offset=0
)

print(f"Retrieved {len(output.output)} items")
print(f"Has more: {output.has_more}")

# Next page
if output.has_more:
    next_page = client.targets.stim.get_output(
        target_name="my_target",
        repo_name="my_repo",
        limit=100,
        offset=100
    )
```

```typescript TypeScript
// First page
let output = await client.targets.stim.getOutput('my_target', {
  repo_name: 'my_repo',
  limit: 100,
  offset: 0,
});

console.log(`Retrieved ${output.output.length} items`);
console.log(`Has more: ${output.has_more}`);

// Next page
if (output.has_more) {
  const nextPage = await client.targets.stim.getOutput('my_target', {
    repo_name: 'my_repo',
    limit: 100,
    offset: 100,
  });
}
```

</CodeGroup>

## Fetch All Results

<CodeGroup>

```python Python
def get_all_output(target_name: str, repo_name: str, output_type: str = "stim"):
    """Fetch all stim or rollout output with pagination"""
    all_output = []
    offset = 0
    limit = 1000  # Maximum allowed
    
    # Select appropriate method
    if output_type == "stim":
        get_fn = lambda: client.targets.stim.get_output(
            target_name, repo_name, limit=limit, offset=offset
        )
    else:
        get_fn = lambda: client.targets.rollout.get_output(
            target_name, repo_name, limit=limit, offset=offset
        )
    
    while True:
        chunk = get_fn()
        all_output.extend(chunk.output)
        
        print(f"Fetched {len(chunk.output)} items (total: {len(all_output)})")
        
        if not chunk.has_more:
            break
        
        offset += limit
    
    return all_output

# Usage
stimuli = get_all_output("my_target", "my_repo", "stim")
trajectories = get_all_output("my_target", "my_repo", "rollout")
```

```typescript TypeScript
async function getAllOutput(
  targetName: string,
  repoName: string,
  outputType: 'stim' | 'rollout' = 'stim'
): Promise<any[]> {
  /**
   * Fetch all stim or rollout output with pagination
   */
  const allOutput: any[] = [];
  let offset = 0;
  const limit = 1000;  // Maximum allowed
  
  while (true) {
    let chunk;
    if (outputType === 'stim') {
      chunk = await client.targets.stim.getOutput(targetName, {
        repo_name: repoName,
        limit,
        offset,
      });
    } else {
      chunk = await client.targets.rollout.getOutput(targetName, {
        repo_name: repoName,
        limit,
        offset,
      });
    }
    
    allOutput.push(...chunk.output);
    
    console.log(`Fetched ${chunk.output.length} items (total: ${allOutput.length})`);
    
    if (!chunk.has_more) {
      break;
    }
    
    offset += limit;
  }
  
  return allOutput;
}

// Usage
const stimuli = await getAllOutput('my_target', 'my_repo', 'stim');
const trajectories = await getAllOutput('my_target', 'my_repo', 'rollout');
```

</CodeGroup>

## Async Pagination

<CodeGroup>

```python Python
import asyncio
from aibread import AsyncBread

async def get_all_output_async(target_name: str, repo_name: str):
    """Async pagination for better performance"""
    async with AsyncBread() as client:
        all_output = []
        offset = 0
        limit = 1000
        
        while True:
            chunk = await client.targets.stim.get_output(
                target_name=target_name,
                repo_name=repo_name,
                limit=limit,
                offset=offset
            )
            
            all_output.extend(chunk.output)
            
            if not chunk.has_more:
                break
            
            offset += limit
        
        return all_output

# Run
output = asyncio.run(get_all_output_async("my_target", "my_repo"))
```

```typescript TypeScript
import Bread from '@aibread/sdk';

async function getAllOutputAsync(
  targetName: string,
  repoName: string
): Promise<any[]> {
  /** Async pagination for better performance */
  const client = new Bread();
  const allOutput: any[] = [];
  let offset = 0;
  const limit = 1000;
  
  while (true) {
    const chunk = await client.targets.stim.getOutput(targetName, {
      repo_name: repoName,
      limit,
      offset,
    });
    
    allOutput.push(...chunk.output);
    
    if (!chunk.has_more) {
      break;
    }
    
    offset += limit;
  }
  
  return allOutput;
}

// Run
const output = await getAllOutputAsync('my_target', 'my_repo');
```

</CodeGroup>

## Generator Pattern

Use generators/iterators for memory-efficient iteration:

<CodeGroup>

```python Python
def iterate_output(target_name: str, repo_name: str, chunk_size: int = 1000):
    """Generator that yields chunks of output"""
    offset = 0
    
    while True:
        chunk = client.targets.stim.get_output(
            target_name=target_name,
            repo_name=repo_name,
            limit=chunk_size,
            offset=offset
        )
        
        yield chunk.output
        
        if not chunk.has_more:
            break
        
        offset += chunk_size

# Usage
for chunk in iterate_output("my_target", "my_repo"):
    for item in chunk:
        process(item)  # Process items without loading all into memory
```

```typescript TypeScript
async function* iterateOutput(
  targetName: string,
  repoName: string,
  chunkSize: number = 1000
): AsyncGenerator<any[], void, unknown> {
  /** Generator that yields chunks of output */
  let offset = 0;
  
  while (true) {
    const chunk = await client.targets.stim.getOutput(targetName, {
      repo_name: repoName,
      limit: chunkSize,
      offset,
    });
    
    yield chunk.output;
    
    if (!chunk.has_more) {
      break;
    }
    
    offset += chunkSize;
  }
}

// Usage
for await (const chunk of iterateOutput('my_target', 'my_repo')) {
  for (const item of chunk) {
    process(item);  // Process items without loading all into memory
  }
}
```

</CodeGroup>

## Progress Tracking

<CodeGroup>

```python Python
def fetch_with_progress(target_name: str, repo_name: str):
    """Fetch output with progress tracking"""
    # Get total count first
    initial = client.targets.stim.get_output(
        target_name=target_name,
        repo_name=repo_name,
        limit=1,
        offset=0
    )
    total_lines = initial.lines
    
    all_output = []
    offset = 0
    limit = 1000
    
    print(f"Total items: {total_lines}")
    
    while True:
        chunk = client.targets.stim.get_output(
            target_name=target_name,
            repo_name=repo_name,
            limit=limit,
            offset=offset
        )
        
        all_output.extend(chunk.output)
        
        # Progress percentage
        progress = (len(all_output) / total_lines) * 100
        print(f"Progress: {progress:.1f}% ({len(all_output)}/{total_lines})")
        
        if not chunk.has_more:
            break
        
        offset += limit
    
    return all_output
```

```typescript TypeScript
async function fetchWithProgress(
  targetName: string,
  repoName: string
): Promise<any[]> {
  /** Fetch output with progress tracking */
  // Get total count first
  const initial = await client.targets.stim.getOutput(targetName, {
    repo_name: repoName,
    limit: 1,
    offset: 0,
  });
  const totalLines = initial.lines;
  
  const allOutput: any[] = [];
  let offset = 0;
  const limit = 1000;
  
  console.log(`Total items: ${totalLines}`);
  
  while (true) {
    const chunk = await client.targets.stim.getOutput(targetName, {
      repo_name: repoName,
      limit,
      offset,
    });
    
    allOutput.push(...chunk.output);
    
    // Progress percentage
    const progress = (allOutput.length / totalLines) * 100;
    console.log(
      `Progress: ${progress.toFixed(1)}% (${allOutput.length}/${totalLines})`
    );
    
    if (!chunk.has_more) {
      break;
    }
    
    offset += limit;
  }
  
  return allOutput;
}
```

</CodeGroup>

## Best Practices

<AccordionGroup>
  <Accordion title="Use Maximum Page Size" icon="maximize">
    Set `limit=1000` to minimize API calls
  </Accordion>

  <Accordion title="Check has_more" icon="circle-check">
    Always check the `has_more` field instead of guessing when to stop
  </Accordion>

  <Accordion title="Use Generators" icon="arrows-rotate">
    For very large datasets, use generators to avoid loading everything into memory
  </Accordion>

  <Accordion title="Handle Errors" icon="triangle-exclamation">
    Implement error handling and retry logic for robust pagination
  </Accordion>

  <Accordion title="Track Progress" icon="chart-line">
    Show progress indicators for better UX when fetching large datasets
  </Accordion>
</AccordionGroup>

## Next Steps

<CardGroup cols={2}>
  <Card
    title="Production Patterns"
    icon="shield"
    href="/sdk/guides/production-patterns"
  >
    Complete workflow examples
  </Card>
  <Card
    title="Advanced Features"
    icon="wand-magic-sparkles"
    href="/sdk/guides/client-configuration"
  >
    Advanced SDK capabilities
  </Card>
</CardGroup>

